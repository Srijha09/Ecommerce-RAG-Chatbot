# Ecommerce-RAG-Chatbot

A production-style **Retrieval-Augmented Generation (RAG)** chatbot that answers customer support questions for **Everstorm Outfitters** using their internal policy PDFs.

The system uses:

- **FAISS** vector store for retrieval  
- **Ollama** LLM for answer generation  
- **A separate Ollama LLM as a judge** to evaluate answers (CORRECT / HALLUCINATION / INCOMPLETE)  
- **FastAPI** backend + **Streamlit** UI  
- **Prometheus-compatible metrics** for basic monitoring  

---

## ðŸŽ¯ Objective

Build an end-to-end RAG system that:

1. Ingests Everstorm policy documents (PDFs)  
2. Chunks and embeds them into a FAISS vector index  
3. Retrieves relevant chunks for each user question  
4. Generates grounded answers with an LLM  
5. Uses **LLM-as-judge** to sanity-check answers and surface evaluation in the UI  
6. Exposes basic metrics for observability  

Target use case: **Customer support** for common questions like returns, refunds, shipping, product care, etc.

---

## ðŸ§± Project Structure

```text
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app: /chat, /health, /metrics
â”‚   â””â”€â”€ schemas.py           # Pydantic request/response models
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ config.py            # Settings (paths, model names, hyperparams)
â”‚   â”œâ”€â”€ data_loader.py       # PDF loading + metadata (source, page)
â”‚   â”œâ”€â”€ chunker.py           # Text splitting / chunking
â”‚   â”œâ”€â”€ vectorstore.py       # FAISS index build/load
â”‚   â”œâ”€â”€ llm.py               # LLM factories (generator + judge)
â”‚   â”œâ”€â”€ pipeline.py          # RAGPipeline.ask() â€“ main RAG logic
â”‚   â”œâ”€â”€ evaluator.py         # Inline LLM-as-judge (CORRECT / HALLUCINATION / INCOMPLETE)
â”‚   â””â”€â”€ eval_metrics.py      # Classic metrics (BLEU / ROUGE / etc., optional)
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py               # Streamlit UI (chat + retrieved context + LLM evaluation)
â”‚
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ metrics.py           # Prometheus metrics: latency, errors, retrieved chunks
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ build_index.py       # Offline script to build the FAISS index from PDFs
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ *.pdf                # Everstorm policy PDFs (input)
â”‚   â””â”€â”€ faiss_index/         # Saved FAISS index (output)
â”‚
â”œâ”€â”€ everstorm_eval_dataset.jsonl  # Optional: eval dataset for offline testing



flowchart LR
    subgraph UI["Streamlit UI (ui/app.py)"]
        U[User] -->|Question| ST[Chat Input]
        ST -->|POST /chat| API
    end

    subgraph Backend["FastAPI Backend (app/main.py)"]
        API[POST /chat] --> PIPE[RAGPipeline.ask()]
        API --> METRICS[Prometheus Metrics]
    end

    subgraph RAG["RAG Core (rag/pipeline.py)"]
        PIPE --> RETRIEVE[FAISS Vector Store]
        RETRIEVE --> DOCS[Top-k Chunks]
        DOCS --> PROMPT[Prompt Builder]
        PROMPT --> GEN_LLM[Generator LLM\n(Ollama)]
        GEN_LLM --> ANSWER[Answer]
    end

    subgraph Store["Index & Data"]
        PDF[Everstorm PDFs] --> LOADER[PDF Loader\n+ Chunker]
        LOADER --> INDEX[FAISS Index]
        INDEX -.-> RETRIEVE
    end

    subgraph Judge["LLM-as-Judge (rag/evaluator.py)"]
        ANSWER --> JPROMPT[Judge Prompt\n(CONTEXT + ANSWER)]
        DOCS --> JPROMPT
        JPROMPT --> JLLM[Judge LLM\n(Ollama, smaller)]
        JLLM --> JL[Label:\nCORRECT / HALLUCINATION / INCOMPLETE]
    end

    ANSWER --> API
    JL --> API

    API -->|JSON: answer + sources + judge_label| UI
    UI -->|Show answer, context, LLM eval| U

â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
