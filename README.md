# Ecommerce-RAG-Chatbot
A production-style **Retrieval-Augmented Generation (RAG)** chatbot that answers customer support questions for **Everstorm Outfitters** using their internal policy PDFs.

The system uses:

- **FAISS** vector store for retrieval  
- **Ollama** LLM for answer generation  
- **A separate Ollama LLM as a judge** to evaluate answers (CORRECT / HALLUCINATION / INCOMPLETE)  
- **FastAPI** backend + **Streamlit** UI  
- **Prometheus-compatible metrics** for basic monitoring  


## Project Structure

```text
.
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI app: /chat, /health, /metrics
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py           # Pydantic request/response models
‚îÇ
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings (paths, model names, hyperparams)
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py       # PDF loading + metadata (source, page)
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py           # Text splitting / chunking
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore.py       # FAISS index build/load
‚îÇ   ‚îú‚îÄ‚îÄ llm.py               # LLM factories (generator + judge)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py          # RAGPipeline.ask() ‚Äì main RAG logic
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py         # Inline LLM-as-judge (CORRECT / HALLUCINATION / INCOMPLETE)
‚îÇ   ‚îî‚îÄ‚îÄ eval_metrics.py      # Classic metrics (BLEU / ROUGE / etc., optional)
‚îÇ
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ app.py               # Streamlit UI (chat + retrieved context + LLM evaluation)
‚îÇ
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py           # Prometheus metrics: latency, errors, retrieved chunks
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ build_index.py       # Offline script to build the FAISS index from PDFs
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ *.pdf                # Everstorm policy PDFs (input)
‚îÇ   ‚îî‚îÄ‚îÄ faiss_index/         # Saved FAISS index (output)
‚îÇ
‚îú‚îÄ‚îÄ everstorm_eval_dataset.jsonl  # Optional: eval dataset for offline testing
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

```mermaid
flowchart LR

    %% UI Layer
    U[User]
    UI[Streamlit UI]
    U --> UI

    %% Backend
    API[FastAPI Chat Endpoint]
    UI -->|POST chat| API

    %% RAG Pipeline
    RAG[RAG Pipeline]
    RET[Retriever]
    CHUNKS[Top K Chunks]
    PROMPT[Prompt Builder]
    GEN[Generator LLM]
    ANSWER[Answer]

    API --> RAG
    RAG --> RET
    RET --> CHUNKS
    CHUNKS --> PROMPT
    PROMPT --> GEN
    GEN --> ANSWER

    %% Index and Data
    PDF[Everstorm PDFs]
    LOADER[PDF Loader and Chunker]
    INDEX[FAISS Index]

    PDF --> LOADER --> INDEX
    INDEX --> RET

    %% Judge
    JUDGE[Judge LLM]
    JPROMPT[Judge Prompt]
    LABEL[Judge Label]

    ANSWER --> JPROMPT
    CHUNKS --> JPROMPT
    JPROMPT --> JUDGE
    JUDGE --> LABEL

    %% Return to UI
    ANSWER --> UI
    LABEL --> UI

```

## Offline Evaluation Benchmark

To measure the reliability and grounding of the RAG system, we evaluated it using a 55-question
Everstorm policy dataset.

Each question includes:

- the user query  
- ground-truth answer  
- retrieved context  
- model-generated answer  
- evaluation metrics (BLEU, METEOR, ROUGE-L)  
- LLM-as-judge classification  

### üîç Evaluation Methodology

The evaluation pipeline runs:

1. **RAG retrieval** using FAISS
2. **LLM generation** using the main model (llama3.1)
3. **Self-critique LLM-as-Judge**  
   - up to 3 cycles of reflection  
   - labels answer as CORRECT / INCOMPLETE / HALLUCINATION / MAX_CYCLES
4. **Lexical similarity metrics**  
   - BLEU  
   - METEOR  
   - ROUGE-L  
5. **Aggregate metrics** across the dataset

---

### üìä Results Summary

| Metric | Value |
|--------|--------|
| Total Questions | **55** |
| Correct (%) | **74.5%** |
| Hallucination (%) | **9.0%** |
| Incomplete (%) | **12.7%** |
| Max Cycles (%) | **3.6%** |
| Avg BLEU | **0.18** |
| Avg METEOR | **0.11** |
| Avg ROUGE-L | **0.42** |

LLM-as-Judge is the primary metric for correctness because RAG answers are often concise and paraphrased; lexical metrics may show low scores despite semantically correct answers.

---

2. Evaluation Pipeline Flow

The offline evaluation script (scripts/run_offline_eval.py) runs the following steps for each question:

Retrieve documents with RAG

Generate answer with the main LLM

Run LLM-as-Judge

Up to 3 critique cycles

Produces a label: CORRECT, HALLUCINATION, INCOMPLETE, or MAX_CYCLES

Compute BLEU / ROUGE-L / METEOR

Log per-question results

Aggregate final metrics:

% CORRECT

% HALLUCINATION

% INCOMPLETE

Average BLEU / ROUGE / METEOR

Failure / timeout rat






## ‚öôÔ∏è Prerequisites
- **Python 3.10+**  
  (Project tested on Python **3.11** and **3.13**)

- **Ollama** installed and running locally  
  https://ollama.com

- **Required models (pull via Ollama):**
  ```bash
  ollama pull llama3.1          # generator LLM
  ollama pull qwen2.5:0.5b      # judge LLM (fast & small)
## üöÄ Setup & Run

### **1. Clone the repo & create a virtual environment**

```bash
git clone <your-repo-url> Ecommerce-RAG-Chatbot
cd Ecommerce-RAG-Chatbot

python -m venv .venv