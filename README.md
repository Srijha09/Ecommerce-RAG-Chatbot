# Ecommerce-RAG-Chatbot
A production-style **Retrieval-Augmented Generation (RAG)** chatbot that answers customer support questions for **Everstorm Outfitters** using their internal policy PDFs.

The system uses:

- **FAISS** vector store for retrieval  
- **Ollama** LLM for answer generation  
- **A separate Ollama LLM as a judge** to evaluate answers (CORRECT / HALLUCINATION / INCOMPLETE)  
- **FastAPI** backend + **Streamlit** UI  
- **Prometheus-compatible metrics** for basic monitoring  


## Project Structure

```text
.
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI app: /chat, /health, /metrics
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py           # Pydantic request/response models
‚îÇ
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings (paths, model names, hyperparams)
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py       # PDF loading + metadata (source, page)
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py           # Text splitting / chunking
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore.py       # FAISS index build/load
‚îÇ   ‚îú‚îÄ‚îÄ llm.py               # LLM factories (generator + judge)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py          # RAGPipeline.ask() ‚Äì main RAG logic
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py         # Inline LLM-as-judge (CORRECT / HALLUCINATION / INCOMPLETE)
‚îÇ   ‚îî‚îÄ‚îÄ eval_metrics.py      # Classic metrics (BLEU / ROUGE / etc., optional)
‚îÇ
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ app.py               # Streamlit UI (chat + retrieved context + LLM evaluation)
‚îÇ
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py           # Prometheus metrics: latency, errors, retrieved chunks
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ build_index.py       # Offline script to build the FAISS index from PDFs
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ *.pdf                # Everstorm policy PDFs (input)
‚îÇ   ‚îî‚îÄ‚îÄ faiss_index/         # Saved FAISS index (output)
‚îÇ
‚îú‚îÄ‚îÄ everstorm_eval_dataset.jsonl  # Optional: eval dataset for offline testing
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

```mermaid
flowchart LR

    %% UI Layer
    U[User]
    UI[Streamlit UI]
    U --> UI

    %% Backend
    API[FastAPI Chat Endpoint]
    UI -->|POST chat| API

    %% RAG Pipeline
    RAG[RAG Pipeline]
    RET[Retriever]
    CHUNKS[Top K Chunks]
    PROMPT[Prompt Builder]
    GEN[Generator LLM]
    ANSWER[Answer]

    API --> RAG
    RAG --> RET
    RET --> CHUNKS
    CHUNKS --> PROMPT
    PROMPT --> GEN
    GEN --> ANSWER

    %% Index and Data
    PDF[Everstorm PDFs]
    LOADER[PDF Loader and Chunker]
    INDEX[FAISS Index]

    PDF --> LOADER --> INDEX
    INDEX --> RET

    %% Judge
    JUDGE[Judge LLM]
    JPROMPT[Judge Prompt]
    LABEL[Judge Label]

    ANSWER --> JPROMPT
    CHUNKS --> JPROMPT
    JPROMPT --> JUDGE
    JUDGE --> LABEL

    %% Return to UI
    ANSWER --> UI
    LABEL --> UI

```
## ‚öôÔ∏è Prerequisites
- **Python 3.10+**  
  (Project tested on Python **3.11** and **3.13**)

- **Ollama** installed and running locally  
  https://ollama.com

- **Required models (pull via Ollama):**
  ```bash
  ollama pull llama3.1          # generator LLM
  ollama pull qwen2.5:0.5b      # judge LLM (fast & small)
## üöÄ Setup & Run

### **1. Clone the repo & create a virtual environment**

```bash
git clone <your-repo-url> Ecommerce-RAG-Chatbot
cd Ecommerce-RAG-Chatbot

python -m venv .venv